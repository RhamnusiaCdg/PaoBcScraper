import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import json
import base64
import os
import sys
from google.oauth2 import service_account
from googleapiclient.discovery import build
import logging
import re

# ==========================================================
# DEBUG SETTINGS
# ==========================================================
DEBUG_MODE = True

# ==========================================================
# LOGGING SETUP
# ==========================================================
logging.basicConfig(
    level=logging.DEBUG if DEBUG_MODE else logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger(__name__)

# ==========================================================
# ENVIRONMENT SETUP
# ==========================================================
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass

# ==========================================================
# Î¡Î¥Î˜ÎœÎ™Î£Î•Î™Î£
# ==========================================================
CALENDAR_ID = os.environ.get("CALENDAR_ID", "primary")
SCOPES = ["https://www.googleapis.com/auth/calendar"]
BASE_URL = "https://www.paobc.gr/schedule/page/"
MAX_PAGES = 10
REQUEST_TIMEOUT = 15


def normalize_team_name(name):
    if not name:
        return ""
    name = re.sub(r'[^\w\s-]', '', str(name), flags=re.UNICODE)
    suffixes = [" BC", " AKTOR", " ATHENS", " OPAP"]
    for suffix in suffixes:
        name = name.replace(suffix, "")
    name = name.strip().upper()
    name = re.sub(r'\s+', ' ', name)
    return name


def authenticate_google_calendar():
    logger.info("ÎˆÎ»ÎµÎ³Ï‡Î¿Ï‚ Ï„Î±Ï…Ï„ÏŒÏ„Î·Ï„Î±Ï‚ Google Calendar...")
    try:
        if os.getenv('SERVICE_ACCOUNT_KEY'):
            logger.info("Î¦ÏŒÏÏ„Ï‰ÏƒÎ· credentials Î±Ï€ÏŒ environment variable")
            service_account_info = json.loads(
                base64.b64decode(os.getenv('SERVICE_ACCOUNT_KEY'))
            )
            logger.info(f"Service Account Email: {service_account_info.get('client_email')}")
            credentials = service_account.Credentials.from_service_account_info(
                service_account_info,
                scopes=SCOPES
            )
        elif os.path.exists('service-account-key.json'):
            logger.info("Î¦ÏŒÏÏ„Ï‰ÏƒÎ· credentials Î±Ï€ÏŒ Î±ÏÏ‡ÎµÎ¯Î¿")
            with open('service-account-key.json', 'r') as f:
                service_account_info = json.load(f)
            logger.info(f"Service Account Email: {service_account_info.get('client_email')}")
            credentials = service_account.Credentials.from_service_account_file(
                'service-account-key.json',
                scopes=SCOPES
            )
        else:
            raise FileNotFoundError("Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎ±Î½ service account credentials!")
        
        service = build("calendar", "v3", credentials=credentials)
        
        try:
            calendar_list = service.calendarList().list().execute()
            logger.debug("Calendars Ï€Î¿Ï… Î­Ï‡ÎµÎ¹ Ï€ÏÏŒÏƒÎ²Î±ÏƒÎ·:")
            for calendar in calendar_list.get('items', []):
                logger.debug(f"   â€¢ {calendar.get('summary')} (ID: {calendar.get('id')})")
        except Exception as e:
            logger.warning(f"Î”ÎµÎ½ Î¼Ï€ÏŒÏÎµÏƒÎ± Î½Î± Ï€Î¬ÏÏ‰ Ï„Î· Î»Î¯ÏƒÏ„Î± Ï„Ï‰Î½ calendars: {e}")
        
        logger.info("âœ“ Î•Ï€Î¹Ï„Ï…Ï‡Î®Ï‚ Ï„Î±Ï…Ï„Î¿Ï€Î¿Î¯Î·ÏƒÎ·")
        return service
        
    except FileNotFoundError as e:
        logger.error(f"âŒ {e}")
        sys.exit(1)
    except Exception as e:
        logger.error(f"âŒ Î£Ï†Î¬Î»Î¼Î± Ï„Î±Ï…Ï„Î¿Ï€Î¿Î¯Î·ÏƒÎ·Ï‚: {e}")
        sys.exit(1)


def scrape_pao_schedule():
    all_matches = []
    seen_matches = set()
    page = 1
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}
    consecutive_empty_pages = 0

    logger.info(f"ÎˆÎ½Î±ÏÎ¾Î· ÏƒÎ¬ÏÏ‰ÏƒÎ·Ï‚ Î±Ï€ÏŒ {BASE_URL}")

    while page <= MAX_PAGES:
        if page == 1:
            url = "https://www.paobc.gr/schedule/"
        else:
            url = f"{BASE_URL}{page}/"

        logger.debug(f"Î£Î¬ÏÏ‰ÏƒÎ· ÏƒÎµÎ»Î¯Î´Î±Ï‚ {page}: {url}")

        try:
            response = requests.get(url, headers=headers, timeout=REQUEST_TIMEOUT)
            response.raise_for_status()

            soup = BeautifulSoup(response.content, "html.parser")
            matches = soup.find_all("div", class_="game")

            if not matches:
                consecutive_empty_pages += 1
                logger.warning(f"Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎ±Î½ Î±Î³ÏÎ½ÎµÏ‚ ÏƒÏ„Î· ÏƒÎµÎ»Î¯Î´Î± {page}")
                if consecutive_empty_pages >= 2:
                    logger.info(f"Î¤ÎµÏÎ¼Î±Ï„Î¹ÏƒÎ¼ÏŒÏ‚: {consecutive_empty_pages} ÏƒÏ…Î½ÎµÏ‡ÏŒÎ¼ÎµÎ½ÎµÏ‚ ÎºÎµÎ½Î­Ï‚ ÏƒÎµÎ»Î¯Î´ÎµÏ‚")
                    break
                page += 1
                continue
            
            consecutive_empty_pages = 0
            matches_on_page = 0

            for match in matches:
                try:
                    data_div = match.find("div", class_="game__data")
                    header_div = match.find("div", class_="game__header")

                    if not data_div or not header_div:
                        continue

                    competition = data_div.find("div", class_="game__data__league").text.strip()

                    date_div = data_div.find("div", class_="game__data__date")
                    date_spans = date_div.find_all("span")
                    date_text = date_spans[0].text.strip() if len(date_spans) > 0 else ""
                    time_text = date_spans[1].text.strip() if len(date_spans) > 1 else ""

                    venue_div = data_div.find("div", class_="game__data__stadium")
                    venue = venue_div.text.strip() if venue_div else "ÎŸÎ‘ÎšÎ‘"

                    name_div = header_div.find("div", class_="game__header__name")
                    team_spans = name_div.find_all("span")
                    home_team = team_spans[0].text.strip() if len(team_spans) > 0 else ""
                    away_team = team_spans[1].text.strip() if len(team_spans) > 1 else ""

                    match_id = f"{home_team}|{away_team}|{date_text}"

                    if match_id in seen_matches:
                        logger.debug(f"â­ï¸ Î Î±ÏÎ¬Î»ÎµÎ¹ÏˆÎ· Î´Î¹Ï€Î»ÏŒÏ„Ï…Ï€Î¿Ï…: {home_team} vs {away_team}")
                        continue

                    seen_matches.add(match_id)

                    match_data = {
                        "date": date_text,
                        "time": time_text,
                        "home_team": home_team,
                        "away_team": away_team,
                        "competition": competition,
                        "venue": venue,
                    }

                    all_matches.append(match_data)
                    matches_on_page += 1
                    logger.debug(f"Î’ÏÎ­Î¸Î·ÎºÎµ: {home_team} vs {away_team} ÏƒÏ„Î¹Ï‚ {date_text}")

                except AttributeError as e:
                    logger.warning(f"Î£Ï†Î¬Î»Î¼Î± Î±Î½Î¬Î»Ï…ÏƒÎ·Ï‚ Î±Î³ÏÎ½Î±: {e}")
                    continue

            logger.info(f"âœ“ Î£ÎµÎ»Î¯Î´Î± {page}: {matches_on_page} Î±Î³ÏÎ½ÎµÏ‚")
            page += 1

        except requests.Timeout:
            logger.error(f"Timeout ÏƒÏ„Î· ÏƒÎµÎ»Î¯Î´Î± {page} - Î ÏÎ¿ÏƒÏ€Î±Î¸Î¿ÏÎ¼Îµ Ï„Î·Î½ ÎµÏ€ÏŒÎ¼ÎµÎ½Î·...")
            page += 1
            continue
        except requests.RequestException as e:
            logger.error(f"Î£Ï†Î¬Î»Î¼Î± Î´Î¹ÎºÏ„ÏÎ¿Ï… ÏƒÏ„Î· ÏƒÎµÎ»Î¯Î´Î± {page}: {e}")
            if all_matches:
                logger.warning(f"Î£Ï…Î½Î­Ï‡ÎµÎ¹Î± Î¼Îµ {len(all_matches)} Î±Î³ÏÎ½ÎµÏ‚ Ï€Î¿Ï… Î²ÏÎ­Î¸Î·ÎºÎ±Î½ Î¼Î­Ï‡ÏÎ¹ Ï„ÏÏÎ±")
                break
            else:
                logger.error("ÎšÏÎ¯ÏƒÎ¹Î¼Î¿ ÏƒÏ†Î¬Î»Î¼Î±: Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎ±Î½ Î±Î³ÏÎ½ÎµÏ‚")
                sys.exit(1)

    logger.info(f"ğŸ“Š Î£ÏÎ½Î¿Î»Î¿: {len(all_matches)} Î¼Î¿Î½Î±Î´Î¹ÎºÎ¿Î¯ Î±Î³ÏÎ½ÎµÏ‚ Î±Ï€ÏŒ {page-1} ÏƒÎµÎ»Î¯Î´ÎµÏ‚")
    return all_matches


def parse_match_datetime(date_text, time_text):
    try:
        greek_days = ["Î”ÎµÏ…Ï„Î­ÏÎ±", "Î¤ÏÎ¯Ï„Î·", "Î¤ÎµÏ„Î¬ÏÏ„Î·", "Î Î­Î¼Ï€Ï„Î·", "Î Î±ÏÎ±ÏƒÎºÎµÏ…Î®", "Î£Î¬Î²Î²Î±Ï„Î¿", "ÎšÏ…ÏÎ¹Î±ÎºÎ®"]
        for day in greek_days:
            date_text = date_text.replace(day + ",", "").replace(day, "")

        greek_months = {
            "Î™Î±Î½Î¿Ï…Î±ÏÎ¯Î¿Ï…": "January", "Î™Î±Î½": "January",
            "Î¦ÎµÎ²ÏÎ¿Ï…Î±ÏÎ¯Î¿Ï…": "February", "Î¦ÎµÎ²": "February",
            "ÎœÎ±ÏÏ„Î¯Î¿Ï…": "March", "ÎœÎ±Ï": "March",
            "Î‘Ï€ÏÎ¹Î»Î¯Î¿Ï…": "April", "Î‘Ï€Ï": "April",
            "ÎœÎ±ÎÎ¿Ï…": "May", "ÎœÎ¬Î¹": "May",
            "Î™Î¿Ï…Î½Î¯Î¿Ï…": "June", "Î™Î¿Ï…Î½": "June",
            "Î™Î¿Ï…Î»Î¯Î¿Ï…": "July", "Î™Î¿Ï…Î»": "July",
            "Î‘Ï…Î³Î¿ÏÏƒÏ„Î¿Ï…": "August", "Î‘Ï…Î³": "August",
            "Î£ÎµÏ€Ï„ÎµÎ¼Î²ÏÎ¯Î¿Ï…": "September", "Î£ÎµÏ€": "September",
            "ÎŸÎºÏ„Ï‰Î²ÏÎ¯Î¿Ï…": "October", "ÎŸÎºÏ„": "October",
            "ÎÎ¿ÎµÎ¼Î²ÏÎ¯Î¿Ï…": "November", "ÎÎ¿Îµ": "November",
            "Î”ÎµÎºÎµÎ¼Î²ÏÎ¯Î¿Ï…": "December", "Î”ÎµÎº": "December",
        }

        english_months = {
            "January": "01", "February": "02", "March": "03",
            "April": "04", "May": "05", "June": "06",
            "July": "07", "August": "08", "September": "09",
            "October": "10", "November": "11", "December": "12",
            "Jan": "01", "Feb": "02", "Mar": "03",
            "Apr": "04", "Jun": "06", "Jul": "07",
            "Aug": "08", "Sep": "09", "Oct": "10",
            "Nov": "11", "Dec": "12",
        }

        for greek, english in greek_months.items():
            date_text = date_text.replace(greek, english)

        english_days = ["Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"]
        for day in english_days:
            date_text = date_text.replace(day + ",", "").replace(day, "")

        date_text = date_text.strip().replace(",", "")

        parts = date_text.split()
        if len(parts) >= 3:
            day = parts[0]
            month = parts[1]
            year = parts[2]

            month_num = english_months.get(month, month)

            time = time_text.strip()
            if not time or ":" not in time or len(time) > 5:
                time = "21:15"

            datetime_str = f"{day}/{month_num}/{year} {time}"
            match_datetime = datetime.strptime(datetime_str, "%d/%m/%Y %H:%M")

            return match_datetime

        return None

    except Exception as e:
        logger.warning(f"Î£Ï†Î¬Î»Î¼Î± Î±Î½Î¬Î»Ï…ÏƒÎ·Ï‚ Î·Î¼ÎµÏÎ¿Î¼Î·Î½Î¯Î±Ï‚ '{date_text} {time_text}': {e}")
        return None


def create_match_identifier(match_data, match_datetime):
    home_normalized = normalize_team_name(match_data['home_team'])
    away_normalized = normalize_team_name(match_data['away_team'])
    teams_sorted = sorted([home_normalized, away_normalized])
    
    if match_datetime:
        date_part = match_datetime.strftime("%Y-%m-%d")
        return f"{teams_sorted[0]}|{teams_sorted[1]}|{date_part}"
    
    return f"{teams_sorted[0]}|{teams_sorted[1]}"


def extract_teams_from_event(event):
    summary = event.get("summary", "")
    
    logger.debug(f"ğŸ“ EXTRACTING FROM SUMMARY: '{summary}'")
    
    clean_summary = re.sub(r'[^\w\s\-\[\]]', ' ', summary, flags=re.UNICODE)
    clean_summary = re.sub(r'\s+', ' ', clean_summary).strip()
    
    logger.debug(f"ğŸ“ CLEANED SUMMARY: '{clean_summary}'")
    
    if " - " in clean_summary:
        if "[" in clean_summary:
            clean_summary = clean_summary.split("[")[0].strip()
        
        parts = clean_summary.split(" - ")
        if len(parts) >= 2:
            home_team = parts[0].strip()
            away_team = parts[1].strip()
            
            logger.debug(f"ğŸ“ EXTRACTED: '{home_team}' vs '{away_team}'")
            return home_team, away_team
    
    logger.debug(f"ğŸ“ FAILED TO EXTRACT from: '{summary}'")
    return None, None


def get_existing_events_map(calendar_events):
    events_map = {}
    
    logger.info(f"ğŸ” Processing {len(calendar_events)} calendar events...")
    
    for i, event in enumerate(calendar_events, 1):
        try:
            event_start = event.get("start", {}).get("dateTime", "")
            summary = event.get("summary", "")
            
            logger.debug(f"  [{i}] Event: '{summary}' at {event_start}")
            
            if not event_start:
                logger.debug(f"     âš ï¸ No start time, skipping")
                continue
                
            home_team, away_team = extract_teams_from_event(event)
            if not home_team or not away_team:
                logger.debug(f"     âš ï¸ Could not extract teams, skipping")
                continue
            
            home_normalized = normalize_team_name(home_team)
            away_normalized = normalize_team_name(away_team)
            teams_sorted = sorted([home_normalized, away_normalized])
            
            event_datetime = datetime.fromisoformat(
                event_start.replace("Z", "+00:00")
            ).replace(tzinfo=None)
            
            date_key = event_datetime.strftime("%Y-%m-%d")
            event_key = f"{teams_sorted[0]}|{teams_sorted[1]}|{date_key}"
            
            events_map[event_key] = {
                "event": event,
                "datetime": event_datetime,
                "original_home": home_team,
                "original_away": away_team
            }
            
            logger.debug(f"     âœ… Mapped as: {event_key}")
            
        except Exception as e:
            logger.warning(f"Î£Ï†Î¬Î»Î¼Î± ÎµÏ€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î±Ï‚ event: {e}")
            continue
    
    logger.info(f"ğŸ“Š Created events map with {len(events_map)} entries")
    
    logger.debug("ğŸ“‹ All event keys in map:")
    for key in sorted(events_map.keys()):
        logger.debug(f"   - {key}")
    
    return events_map


def create_event_data(match_data, match_datetime):
    date_str = match_datetime.strftime("%d/%m")
    summary = f"â˜˜ï¸ğŸ€ {match_data['home_team']} - {match_data['away_team']} [{date_str}]"
    end_datetime = match_datetime + timedelta(hours=2)
    
    return {
        "summary": summary,
        "location": match_data.get("venue", "ÎŸÎ‘ÎšÎ‘"),
        "description": f"Î”Î¹Î¿ÏÎ³Î¬Î½Ï‰ÏƒÎ·: {match_data.get('competition', 'Î•Î»Î»Î·Î½Î¹ÎºÏŒ Î ÏÏ‰Ï„Î¬Î¸Î»Î·Î¼Î±')}",
        "start": {
            "dateTime": match_datetime.isoformat(),
            "timeZone": "Europe/Athens",
        },
        "end": {
            "dateTime": end_datetime.isoformat(),
            "timeZone": "Europe/Athens",
        },
        "reminders": {
            "useDefault": False,
            "overrides": [
                {"method": "popup", "minutes": 60},
            ],
        },
    }


def sync_match_with_calendar(service, match_data, existing_events_map):
    try:
        match_datetime = parse_match_datetime(match_data["date"], match_data["time"])
        if not match_datetime:
            logger.warning(f"Î Î±ÏÎ¬Î»ÎµÎ¹ÏˆÎ· Î±Î³ÏÎ½Î±: {match_data['home_team']} vs {match_data['away_team']}")
            return False, None

        home_normalized = normalize_team_name(match_data['home_team'])
        away_normalized = normalize_team_name(match_data['away_team'])
        teams_sorted = sorted([home_normalized, away_normalized])
        date_key = match_datetime.strftime("%Y-%m-%d")
        match_key = f"{teams_sorted[0]}|{teams_sorted[1]}|{date_key}"
        
        logger.debug(f"ğŸ”‘ Match key: {match_key}")
        
        event_data = create_event_data(match_data, match_datetime)
        
        if match_key in existing_events_map:
            existing_info = existing_events_map[match_key]
            existing_event = existing_info["event"]
            existing_datetime = existing_info["datetime"]
            
            time_diff = abs((existing_datetime - match_datetime).total_seconds())
            
            if time_diff < 60:
                logger.debug(f"â„¹ï¸ Î¥Ï€Î¬ÏÏ‡ÎµÎ¹ Î®Î´Î·: {match_data['home_team']} vs {match_data['away_team']}")
                return True, match_key
            else:
                existing_time_str = existing_datetime.strftime("%H:%M")
                new_time_str = match_datetime.strftime("%H:%M")
                
                service.events().update(
                    calendarId=CALENDAR_ID, 
                    eventId=existing_event["id"], 
                    body=event_data
                ).execute()
                
                logger.info(f"ğŸ”„ Î•ÎÎ—ÎœÎ•Î¡Î©Î£Î—: {match_data['home_team']} vs {match_data['away_team']} ({existing_time_str} â†’ {new_time_str})")
                return True, match_key
        else:
            service.events().insert(
                calendarId=CALENDAR_ID, 
                body=event_data
            ).execute()
            
            logger.info(f"âœ… Î Î¡ÎŸÎ£Î˜Î—ÎšÎ—: {match_data['home_team']} vs {match_data['away_team']} ({match_datetime.strftime('%d/%m/%Y %H:%M')})")
            return True, match_key
            
    except Exception as e:
        logger.error(f"Î£Ï†Î¬Î»Î¼Î± ÏƒÏ…Î³Ï‡ÏÎ¿Î½Î¹ÏƒÎ¼Î¿Ï Î±Î³ÏÎ½Î±: {e}")
        return False, None


def get_all_pao_events(service):
    try:
        time_min = (datetime.now() - timedelta(days=180)).isoformat() + "Z"
        time_max = (datetime.now() + timedelta(days=540)).isoformat() + "Z"

        logger.info(f"Î‘Î½Î±Î¶Î®Ï„Î·ÏƒÎ· events Î±Ï€ÏŒ {(datetime.now() - timedelta(days=180)).strftime('%d/%m/%Y')} Î­Ï‰Ï‚ {(datetime.now() + timedelta(days=540)).strftime('%d/%m/%Y')}")

        events_result = (
            service.events()
            .list(
                calendarId=CALENDAR_ID,
                timeMin=time_min,
                timeMax=time_max,
                maxResults=2500,
                singleEvents=True,
                orderBy="startTime",
            )
            .execute()
        )

        all_events = events_result.get("items", [])

        pao_events = []
        for event in all_events:
            summary = event.get("summary", "")
            if ("ğŸ€" in summary or "â˜˜ï¸" in summary or "Î Î‘ÎŸ" in summary.upper() or 
                "PANATHINAIKOS" in summary.upper() or 
                (" - " in summary and any(x in summary.upper() for x in ["VS", "VS.", "Î‘Î“Î©ÎÎ‘Î£", "Î‘Î“Î©ÎÎ‘"]))):
                pao_events.append(event)

        logger.info(f"âœ“ Î’ÏÎ­Î¸Î·ÎºÎ±Î½ {len(pao_events)} PAO basketball events")
        return pao_events

    except Exception as e:
        logger.error(f"Î£Ï†Î¬Î»Î¼Î± Î±Î½Î¬ÎºÏ„Î·ÏƒÎ·Ï‚ events: {e}")
        return []


def delete_obsolete_events(service, website_match_keys, existing_events_map):
    deleted_count = 0
    
    logger.info(f"ğŸ” ÎˆÎ»ÎµÎ³Ï‡Î¿Ï‚ {len(existing_events_map)} events Î³Î¹Î± Î´Î¹Î±Î³ÏÎ±Ï†Î®...")
    logger.info(f"ğŸ“‹ Keys Î±Ï€ÏŒ website: {len(website_match_keys)}")
    
    logger.debug("=== Website Match Keys ===")
    for key in sorted(website_match_keys):
        logger.debug(f"  - {key}")
    
    logger.debug("\n=== Calendar Event Keys ===")
    for key in sorted(existing_events_map.keys()):
        logger.debug(f"  - {key}")
    
    for event_key, event_info in list(existing_events_map.items()):
        if event_key not in website_match_keys:
            try:
                event = event_info["event"]
                event_datetime = event_info["datetime"]
                home_team = event_info.get("original_home", "Unknown")
                away_team = event_info.get("original_away", "Unknown")
                
                logger.info(f"ğŸ¯ Î˜Î± Î´Î¹Î±Î³ÏÎ±Ï†ÎµÎ¯: {event_key}")
                logger.info(f"   Teams: {home_team} vs {away_team}")
                logger.info(f"   Time: {event_datetime.strftime('%d/%m/%Y %H:%M')}")
                
                service.events().delete(
                    calendarId=CALENDAR_ID, 
                    eventId=event["id"]
                ).execute()
                
                logger.info(f"ğŸ—‘ï¸ Î”Î™Î‘Î“Î¡Î‘Î¦Î—: {home_team} vs {away_team} ({event_datetime.strftime('%d/%m/%Y %H:%M')})")
                deleted_count += 1
                
                del existing_events_map[event_key]
                
            except Exception as e:
                logger.warning(f"Î£Ï†Î¬Î»Î¼Î± Î´Î¹Î±Î³ÏÎ±Ï†Î®Ï‚: {e}")
                continue
        else:
            logger.debug(f"âœ… ÎšÏÎ±Ï„Î¬Î¼Îµ: {event_key}")
    
    return deleted_count


def main():
    logger.info("=" * 70)
    logger.info("ğŸ€ Panathinaikos BC Schedule Scraper - ÎˆÎ½Î±ÏÎ¾Î·")
    logger.info(f"ğŸ“… CALENDAR_ID: {CALENDAR_ID}")
    
    if os.getenv('SERVICE_ACCOUNT_KEY'):
        logger.info("ğŸ”‘ Î§ÏÎ®ÏƒÎ·: GitHub Secrets")
    elif os.path.exists('service-account-key.json'):
        with open('service-account-key.json', 'r') as f:
            sa_info = json.load(f)
        logger.info(f"ğŸ”‘ Î§ÏÎ®ÏƒÎ·: Local File - {sa_info.get('client_email')}")
    else:
        logger.warning("âš ï¸ Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎ±Î½ credentials!")
    
    logger.info("=" * 70)

    service = authenticate_google_calendar()

    logger.info("\nğŸ“… Î¦Î‘Î£Î— 0: Î‘Î½Î¬ÎºÏ„Î·ÏƒÎ· Ï…Ï€Î±ÏÏ‡ÏŒÎ½Ï„Ï‰Î½ events...")
    logger.info("-" * 70)
    existing_calendar_events = get_all_pao_events(service)
    existing_events_map = get_existing_events_map(existing_calendar_events)

    logger.info(f"\nğŸŒ Î¦Î‘Î£Î— 1: Î£Î¬ÏÏ‰ÏƒÎ· Ï€ÏÎ¿Î³ÏÎ¬Î¼Î¼Î±Ï„Î¿Ï‚ Î±Ï€ÏŒ paobc.gr...")
    logger.info("-" * 70)
    website_matches = scrape_pao_schedule()

    if not website_matches:
        logger.error("âŒ Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎ±Î½ Î±Î³ÏÎ½ÎµÏ‚ - Ï„ÎµÏÎ¼Î±Ï„Î¹ÏƒÎ¼ÏŒÏ‚")
        sys.exit(1)

    logger.info(f"\nğŸ”„ Î¦Î‘Î£Î— 2: Î£Ï…Î³Ï‡ÏÎ¿Î½Î¹ÏƒÎ¼ÏŒÏ‚ {len(website_matches)} Î±Î³ÏÎ½Ï‰Î½...")
    logger.info("-" * 70)

    synced_count = 0
    website_match_keys = set()
    
    for match in website_matches:
        synced, match_key = sync_match_with_calendar(service, match, existing_events_map)
        if synced and match_key:
            synced_count += 1
            website_match_keys.add(match_key)
    
    logger.info(f"Î£Ï…Î³Ï‡ÏÎ¿Î½Î¯ÏƒÏ„Î·ÎºÎ±Î½ {synced_count} Î±Ï€ÏŒ {len(website_matches)} Î±Î³ÏÎ½ÎµÏ‚")
    logger.info(f"ÎœÎ¿Î½Î±Î´Î¹ÎºÎ¬ keys: {len(website_match_keys)}")

    logger.info(f"\nğŸ—‘ï¸ Î¦Î‘Î£Î— 3: ÎˆÎ»ÎµÎ³Ï‡Î¿Ï‚ Î³Î¹Î± Î´Î¹Î±Î³ÏÎ±Ï†Î­Ï‚...")
    logger.info("-" * 70)

    deleted_count = delete_obsolete_events(service, website_match_keys, existing_events_map)

    logger.info("\n" + "=" * 70)
    logger.info("âœ… ÎŸÎ›ÎŸÎšÎ›Î—Î¡Î©Î˜Î—ÎšÎ•!")
    logger.info(f"   â€¢ Î‘Î³ÏÎ½ÎµÏ‚ ÏƒÏ„Î¿ site: {len(website_matches)}")
    logger.info(f"   â€¢ Î£Ï…Î³Ï‡ÏÎ¿Î½Î¯ÏƒÏ„Î·ÎºÎ±Î½: {synced_count}")
    logger.info(f"   â€¢ Î”Î¹Î±Î³ÏÎ¬Ï†Î·ÎºÎ±Î½: {deleted_count}")
    logger.info(f"   â€¢ Î¥Ï€ÏŒÎ»Î¿Î¹Ï€Î± events: {len(existing_events_map)}")
    logger.info("=" * 70)


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        logger.warning("\nâš ï¸ Î”Î¹Î±ÎºÏŒÏ€Î·ÎºÎµ Î±Ï€ÏŒ Ï„Î¿Î½ Ï‡ÏÎ®ÏƒÏ„Î·")
        sys.exit(0)
    except Exception as e:
        logger.error(f"\nâŒ ÎšÏÎ¯ÏƒÎ¹Î¼Î¿ ÏƒÏ†Î¬Î»Î¼Î±: {e}", exc_info=True)
        sys.exit(1)